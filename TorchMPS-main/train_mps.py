#!/usr/bin/env python3
"""
train_mps_paths.py

Train an MPS on path-augmented data constructed from the Chebyshev data
generated by poly_teacher.py.

For each base input x (from the original Chebyshev grid), we generate
additional points of the form:

    x(t) = t * x

for t in a set of Chebyshev nodes in [0, 1], using the SAME
chebyshev_nodes_unit_interval function as later used in TN-SHAP
Vandermonde interpolation.

We then set the target as:

    y(t) = teacher(x(t))

So the MPS is explicitly trained on the exact t-grid that TN-SHAP will probe.

We also use the *entire* augmented dataset for training (no held-out test set).
"""

import time
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

from torchmps import MPS
from poly_teacher import load_teacher, load_data

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------
# Config
# -----------------------
PREFIX = "poly"       # must match --prefix in poly_teacher.py
BATCH_SIZE = 1000
NUM_EPOCHS = 300      # Increased for better convergence
BOND_DIM = 50
LEARN_RATE = 1e-3
L2_REG = 0
GRAD_CLIP = 1.0       # Gradient clipping threshold
EARLY_STOP_PATIENCE = 50  # Stop if no improvement for this many epochs
LR_PATIENCE = 20      # Reduce LR if no improvement for this many epochs
LR_FACTOR = 0.5       # Multiply LR by this factor when reducing
MIN_LR = 1e-7         # Don't reduce LR below this

# Number of t-nodes along the path (should match what you use in TN-SHAP)
N_T_NODES = 10


def r2_score(y_true, y_pred) -> float:
    y_true = y_true.detach()
    y_pred = y_pred.detach()
    var = torch.var(y_true)
    if var < 1e-12:
        return 1.0 if torch.allclose(y_true, y_pred) else 0.0
    return float(1.0 - torch.mean((y_true - y_pred) ** 2) / (var + 1e-12))


def chebyshev_nodes_unit_interval(n_nodes: int, device=None, dtype=torch.float32):
    """
    Chebyshev nodes of the first kind, mapped from [-1, 1] to [0, 1].

    Standard Chebyshev nodes in [-1, 1]:
        u_k = cos((2k-1)/(2n) * pi),  k=1..n

    Map to t in [0,1] via: t = (u + 1) / 2.
    """
    if device is None:
        device = DEVICE
    k = torch.arange(1, n_nodes + 1, dtype=torch.float64, device=device)
    u = torch.cos((2.0 * k - 1.0) / (2.0 * n_nodes) * torch.pi)  # [-1,1]
    t = (u + 1.0) / 2.0  # [0,1]
    return t.to(dtype)


def augment_with_one(x: torch.Tensor) -> torch.Tensor:
    """
    x: (N, D) -> (N, D+1) with a leading 1 column.
    """
    ones = torch.ones(x.shape[0], 1, device=x.device, dtype=x.dtype)
    return torch.cat([ones, x], dim=1)


# ... imports and config exactly as you wrote ...

def main():
    print(f"Device: {DEVICE}")

    # Load teacher and original Chebyshev data
    teacher = load_teacher(PREFIX)
    x_train, y_train, x_test, y_test = load_data(PREFIX)

    # Try to get the true polynomial degree from the teacher
    if hasattr(teacher, "deg"):
        true_degree = int(teacher.deg)
    elif hasattr(teacher, "degree"):
        true_degree = int(teacher.degree)
    else:
        # fallback: use number of active features or a small upper bound
        true_degree = len(getattr(teacher, "S", [])) or 5

    # Merge train and test to get the full Chebyshev grid as base points
    x_base = torch.cat([x_train, x_test], dim=0).to(DEVICE)  # (N_base_total, D)
    MAX_BASE = 10
    x_base = x_base[:MAX_BASE]  # we'll use these as TN-SHAP target points as well
    N_base, D = x_base.shape
    print(f"Loaded base data (merged train+test): N_base={N_base}, D={D}")

    # Build Chebyshev nodes in t \in [0,1], same as TN-SHAP will use
    t_nodes = chebyshev_nodes_unit_interval(N_T_NODES, device=DEVICE, dtype=x_base.dtype)
    print(f"Using N_T_NODES={N_T_NODES} Chebyshev nodes in t for path augmentation.")

    # ---- Construct path-augmented dataset for TN-SHAP (h + g_i paths) ----
    X_list = []
    Y_list = []

    teacher.eval()
    with torch.no_grad():
        for b in range(N_base):
            x0 = x_base[b]           # (D,)

            for t in t_nodes:
                # h-path: all features scaled by t
                x_h = t * x0         # (D,)
                y_h = teacher(x_h.unsqueeze(0)).squeeze(0)
                if y_h.ndim > 0:
                    y_h = y_h.squeeze(-1)
                X_list.append(x_h.unsqueeze(0))
                Y_list.append(y_h.unsqueeze(0))

                # g_i paths: feature i clamped, others scaled
                for i in range(D):
                    x_g = t * x0.clone()
                    x_g[i] = x0[i]   # clamp feature i
                    y_g = teacher(x_g.unsqueeze(0)).squeeze(0)
                    if y_g.ndim > 0:
                        y_g = y_g.squeeze(-1)
                    X_list.append(x_g.unsqueeze(0))
                    Y_list.append(y_g.unsqueeze(0))

    X_all = torch.cat(X_list, dim=0)  # ((1 + D) * N_base * N_T_NODES, D)
    Y_all = torch.cat(Y_list, dim=0)  # same length

    print(
        f"Path-augmented dataset size (h + g_i paths): "
        f"{X_all.shape[0]} points = (1 + D)*N_base*N_T_NODES"
    )

    # Append a leading 1 to each input for MPS
    X_all_aug = augment_with_one(X_all)  # (N_all, D+1)
    D_aug = X_all_aug.shape[1]

    train_ds = TensorDataset(X_all_aug, Y_all)
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,
                              shuffle=False, drop_last=False)

    # MPS regressor
    mps = MPS(
        input_dim=D_aug,
        output_dim=1,
        bond_dim=BOND_DIM,
        adaptive_mode=False,
        periodic_bc=False,
    ).to(DEVICE)

    loss_fun = nn.MSELoss()
    optimizer = torch.optim.Adam(mps.parameters(), lr=LEARN_RATE, weight_decay=L2_REG)
    
    # Learning rate scheduler: reduce LR when R² plateaus
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='max',           # maximize R²
        factor=LR_FACTOR,     # reduce LR by this factor
        patience=LR_PATIENCE, # wait this many epochs before reducing
        verbose=True,
        min_lr=MIN_LR,
        threshold=1e-4,       # minimum change to count as improvement
    )

    print(
        f"\nTraining MPS on path-augmented sparse polynomial regression\n"
        f"D_aug={D_aug}, bond_dim={BOND_DIM}, "
        f"N_train={X_all_aug.shape[0]} (no held-out test set)\n"
        f"Learning rate scheduling: reduce by {LR_FACTOR} if no improvement for {LR_PATIENCE} epochs\n"
        f"Early stopping: patience = {EARLY_STOP_PATIENCE} epochs\n"
    )

    start_time = time.time()
    
    # Track best model for early stopping
    best_r2 = -float('inf')
    best_mse = float('inf')
    best_epoch = 0
    best_state = None
    epochs_no_improve = 0

    for epoch in range(1, NUM_EPOCHS + 1):
        mps.train()
        train_loss = 0.0
        n_seen = 0
        grad_norms = []

        for xb, yb in train_loader:
            xb = xb.to(DEVICE)
            yb = yb.to(DEVICE)

            preds = mps(xb).squeeze(-1)
            loss = loss_fun(preds, yb)

            optimizer.zero_grad()
            loss.backward()
            
            # Gradient clipping for stability
            total_norm = torch.nn.utils.clip_grad_norm_(mps.parameters(), GRAD_CLIP)
            grad_norms.append(total_norm.item())
            
            optimizer.step()

            train_loss += loss.item() * xb.size(0)
            n_seen += xb.size(0)

        train_mse = train_loss / max(n_seen, 1)
        avg_grad_norm = sum(grad_norms) / len(grad_norms) if grad_norms else 0.0

        # Monitor R2 on whole TN-SHAP training set
        mps.eval()
        with torch.no_grad():
            preds_all = mps(X_all_aug).squeeze(-1)
            train_r2 = r2_score(Y_all, preds_all)
            
            # Additional diagnostics: per-path MSE
            residuals = (Y_all - preds_all) ** 2
            max_residual = torch.max(residuals).item()
            median_residual = torch.median(residuals).item()

        # Learning rate scheduling based on R²
        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step(train_r2)
        
        # Early stopping logic
        if train_r2 > best_r2 + 1e-6:  # small threshold to avoid numerical noise
            best_r2 = train_r2
            best_mse = train_mse
            best_epoch = epoch
            best_state = {k: v.cpu().clone() for k, v in mps.state_dict().items()}
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        print(f"### Epoch {epoch:03d} ###")
        print(f"Train MSE: {train_mse:.6e} | R²: {train_r2:.6f}")
        print(f"Max residual: {max_residual:.6e} | Median residual: {median_residual:.6e}")
        print(f"Avg grad norm: {avg_grad_norm:.4f} | LR: {current_lr:.2e}")
        print(f"Best R²: {best_r2:.6f} (epoch {best_epoch}) | No improve: {epochs_no_improve}/{EARLY_STOP_PATIENCE}")
        print(f"Runtime so far: {int(time.time() - start_time)} sec\n")
        
        # Early stopping check
        if epochs_no_improve >= EARLY_STOP_PATIENCE:
            print(f"\n⚠️  Early stopping triggered! No improvement for {EARLY_STOP_PATIENCE} epochs.")
            print(f"Restoring best model from epoch {best_epoch} (R² = {best_r2:.6f})")
            mps.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})
            break
    
    # Restore best model if we completed all epochs
    if best_state is not None and epochs_no_improve < EARLY_STOP_PATIENCE:
        print(f"\n✓ Training completed. Restoring best model from epoch {best_epoch} (R² = {best_r2:.6f})")
        mps.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})

    # ---- Save TN-SHAP targets & training dataset & degree info ----
    torch.save(
        {
            "x_base": x_base.detach().cpu(),   # (N_base, D) – TN-SHAP evaluation points
            "t_nodes": t_nodes.detach().cpu(), # (N_T_NODES,) – Chebyshev nodes in [0,1]
            "X_all": X_all.detach().cpu(),     # ((1 + D)*N_base*N_T_NODES, D)
            "Y_all": Y_all.detach().cpu(),     # same length
            "max_degree": true_degree,         # true polynomial degree (or best guess)
        },
        f"{PREFIX}_tnshap_targets.pt",
    )
    print(f"Saved TN-SHAP targets and t-nodes to {PREFIX}_tnshap_targets.pt")

    # ---- Save trained MPS ----
    torch.save(
        {
            "state_dict": mps.state_dict(),
            "D_aug": D_aug,
            "bond_dim": BOND_DIM,
        },
        f"{PREFIX}_mps.pt",
    )
    print(f"Saved trained MPS to {PREFIX}_mps.pt")
    print("Done.")

if __name__ == "__main__":
    main()
